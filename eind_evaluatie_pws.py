# -*- coding: utf-8 -*-
"""eind evaluatie pws.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ej2yRuJsOMRwiXBPNuB7Yf-XpHzLjrmf

https://www.kaggle.com/code/mustisid/solving-sudoku-using-cnn
"""

import tensorflow as tf
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
import keras
from keras.models import load_model
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Conv2D, BatchNormalization, Dense, Flatten, Reshape, MaxPooling2D, Dropout, Activation
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import copy



device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

#sample maken van grotere dataset
df = pd.read_csv('/content/1 mil 9x9 sudoku.csv', dtype='str')

sample_df  = df.sample(500000)
sample_df.to_csv("500k 9x9 sudokus.csv")

sample_df.head()

df.head()
df.columns

df = pd.read_csv('/content/10k 9x9 sudokus.csv', dtype='str') #data laden

que = df['quizzes'].values
soln = df['solutions']

te_deleten_labels= []
te_deleten_feat= []
feat = []
label = []

for i in que:


    try:
      x = np.array([int(j) for j in i]).reshape((9,9,1))
      feat.append(x)

    except:
      print(len(x))
      print("zeker niet true")
      ind = len(feat)
      te_deleten_labels.append(ind)

for i in soln:
  try:
    a = len(label) + 1
    if a not in te_deleten_labels:
      x = np.array([int(j) for j in i]).reshape((81,1)) - 1 #81
      label.append(x)
    else:
      print(True)
      te_deleten_labels.remove(a)

  except:
    print(len(x))
    print('ook true')
    ind = len(label)
    print(len(feat))
    feat.pop(ind)
    print(len(feat))

print(feat[0].shape)
feat = np.array(feat)
feat = feat/9
feat -= .5

print(feat[0].shape)
label = np.array(label)
label = to_categorical(label)

print(feat[0].shape)
#splitting data
x_train, x_temp, y_train, y_temp = train_test_split(feat, label, test_size=0.3, random_state=42)
x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)

def get_model():
  with tf.device('/device:GPU:0'):
    model = keras.models.Sequential()

    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', input_shape=(9,9,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    model.add(Conv2D(128, kernel_size=(1,1), activation='relu', padding='same'))

    model.add(Flatten())
    model.add(Dense(81*9))
    model.add(Reshape((-1, 9)))
    model.add(Activation('softmax'))

    return model

model = get_model()

def get_model():
  with tf.device('/device:GPU:0'):
    model = keras.models.Sequential()

    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', input_shape=(9,9,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    model.add(Conv2D(128, kernel_size=(1,1), activation='relu', padding='same'))

    model.add(Flatten())
    model.add(Dense(81*9))
    model.add(Reshape((-1, 9)))
    model.add(Activation('softmax'))

    return model

model = get_model()


#train model
#opt = tf.keras.optimizers.Adam(learning_rate=.001, beta_1=0.9)
opt = keras.optimizers.Adam(learning_rate=.001)
#####opt = keras.optimizers.SGD(lr=0.015, momentum=0.9)
#opt = keras.optimizers.SGD()
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])

#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15) #optie 1
es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=7, min_delta=0.01) #optie 2
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=200,  callbacks=[es, mc])
# load the saved model
saved_model = load_model('best_model.h5')

# evaluate model
_, train_acc = saved_model.evaluate(x_train, y_train, verbose=0)
_, val_acc = saved_model.evaluate(x_val, y_val, verbose=0)
print('Train: %.3f, Validation: %.3f' % (train_acc, val_acc))

# plot loss during training
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='val')
pyplot.legend()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='val')
pyplot.legend()
pyplot.show()

#train and evaluate model
opt = keras.optimizers.Adam(learning_rate=.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])

es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=7, min_delta=0.01) #optie 2
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=200,  callbacks=[es, mc])
saved_model = load_model('best_model.h5')

# evaluate model
_, train_acc = saved_model.evaluate(x_train, y_train, verbose=0)
_, val_acc = saved_model.evaluate(x_val, y_val, verbose=0)
_, test_acc = saved_model.evaluate(x_test, y_test, verbose=0)
print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))

# plot loss during training
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='val')
pyplot.legend()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='val')
pyplot.legend()
pyplot.show()

#evaluate op of alle getallen in sudoku goed zijn
def indices_to_sudoku(predictions):
    sudoku_solutions = np.argmax(predictions, axis=-1) + 1  # Veronderstel dat 0-indexed is, dus voeg 1 toe
    return sudoku_solutions.reshape(9, 9, -1)
def check_solution(solution, actual_solution):
    return np.array_equal(solution, actual_solution)

predicted_sudokus = []
actual_sudokus = []
correct_count = 0
# x_test zijn de invoergegevens en y_test zijn de daadwerkelijke oplossingen

predictions = model.predict(x_test)
for answer in predictions:
  predicted_sudokus.append(indices_to_sudoku(answer))

for puzzle in (y_test):
  actual_sudokusa = puzzle * 9
  actual_sudokus.append(indices_to_sudoku(actual_sudokusa))

for predicted, actual in zip(predicted_sudokus, actual_sudokus ):
    if check_solution(predicted, actual):
        correct_count += 1

print(predicted_sudokus[0])
print(actual_sudokus[0]) #om te checken of predicted en actual gelijke vorm is

accuracy = correct_count / len(actual_sudokus)
print(f"Modelnauwkeurigheid: {accuracy * 100:.2f}%")

print(correct_count)
print(len(actual_sudokus))

for i in range (0,9):
  ls = []
  for j in range(0,9):
    ls.append(predicted_sudokus[1][i][j])
  print(ls)
print()
for i in range(0,9):
  ls = []
  for j in range(0,9):
    ls.append(actual_sudokus[1][i][j])
  print(ls)
  #print(predicted_sudokus[i])
  #print(actual_sudokus[i])

def denorm(a):

    return (a+.5)*9
def norm(a):

    return (a/9)-.5

def inference_sudoku(sample):
  '''
        This function solves the sudoku by filling blank positions one by one.
  '''
  feat = copy.copy(sample)

  while(1):

        #predicting values
        out = model.predict(feat.reshape((1,9,9,1)), verbose=0) #verbose is hoeveel info wordt getoond
        out = out.squeeze()

        #getting predicted values
        pred = np.argmax(out, axis=1).reshape((9,9))+1
        #getting probablity of each values
        prob = np.around(np.max(out, axis=1).reshape((9,9)), 2)
#creating mask for blank values
        feat = denorm(feat).reshape((9,9))
        #i.e it will give you a 2d array with True/1 and False/0 where 0 is found and where 0 is not found respectively
        mask = (feat==0)

        #if there are no 0 values left than break
        if(mask.sum()==0):
            break

        #getting probablities of values where 0 is present that is our blank values we need to fill
        prob_new = prob*mask

        #getting highest probablity index
        ind = np.argmax(prob_new)
        #getting row and col
        x, y = (ind//9), (ind%9)

         #getting predicted value at that cell
        #assigning that value
        feat[x][y] = pred [x][y]
        #again passing this sudoku with one value added to model to get next most confident value
        feat = norm(feat)
        return pred

def solve_sudoku(game):

    game = game.replace('\n', '')
    game = game.replace(' ', '')
    game = np.array([int(j) for j in game]).reshape((9,9,1))
    game = norm(game)
    game = inference_sudoku(game)
    return game

game = '''
          0 8 0 0 3 2 0 0 1
          7 0 3 0 8 0 0 0 2
          5 0 0 0 0 7 0 3 0
          0 5 0 0 0 1 9 7 0
          6 0 0 7 0 9 0 0 8
          0 4 7 2 0 0 0 5 0
          0 2 0 6 0 0 0 0 9
          8 0 0 0 9 0 3 0 5
          3 0 0 8 2 0 0 1 0
      '''

game = solve_sudoku(game)

print('solved puzzle:\n')
# print(game)
for i in game:
    print(i)
print('\n')

np.sum(game, axis=1) #als het helemaal klopt moet alles 45 zijn. Dit is een chack